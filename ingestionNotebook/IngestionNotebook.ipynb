{"cells":[{"cell_type":"markdown","source":["**Notebook parameters, do not edit**"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"719e4a47-111b-4d8e-8788-816a56f93b1a"},{"cell_type":"code","source":["#period to process\n","period = 0\n","#service principal\n","tenant_id = \"\"\n","client_id = \"\"\n","client_secret = \"\"\n","#customer info\n","billing_account_id = \"\"\n","customer_name = \"\"\n","#lakehouse\n","lakehouse_cost_table = \"\"\n","lakehouse_log_table = \"\"\n","#warehouse\n","warehouse_name = \"\"\n","warehouse_schema = \"\"\n","warehouse_cost_table = \"\"\n"],"outputs":[],"execution_count":null,"metadata":{"tags":["parameters"],"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a0d31653-790f-41fa-bb31-a4730b24f54d"},{"cell_type":"markdown","source":["**Libraries and configuration of the notebook**\n","\n","Modify variables of Lakehouse and/or Warehouse if required"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"9e20d0ea-17cb-42de-ba7d-199553e7fb82"},{"cell_type":"code","source":["# ==========================\n","# BIBLIOTECAS\n","# ==========================\n","\n","import requests\n","import json\n","import datetime\n","import uuid\n","import time\n","from azure.identity import ClientSecretCredential\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType, DateType, FloatType\n","from delta.tables import DeltaTable\n","from pyspark.sql.functions import lit,col,to_date,regexp_extract,current_date, year, month, dayofmonth\n","\n","# ==========================\n","# CONFIGURACIÓN\n","# ==========================\n","\n","\n","# Inicializar Spark\n","spark = SparkSession.builder.getOrCreate()\n","\n","# ==========================\n","# 1. Calcular periodo y autenticar\n","# ==========================\n","current_date = datetime.datetime.now() + datetime.timedelta(days=period * 30)\n","period_name = current_date.strftime(\"%Y%m\")\n","\n","try:\n","    credential = ClientSecretCredential(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret)\n","    token = credential.get_token(\"https://management.azure.com/.default\").token\n","except Exception as e:\n","    raise Exception(f\"Error en autenticación: {e}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"25e1c423-89b7-448a-aae9-8afc50027e7b"},{"cell_type":"markdown","source":["**Initialize export of the data using the generateCostDetailsReport API and log the status**"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"cc8cc4dd-bc1f-4080-8d5c-1ea91a86b4b1"},{"cell_type":"code","source":["# ==========================\n","# 2. Iniciar exportación y registrar log\n","# ==========================\n","uri = f\"https://management.azure.com/providers/Microsoft.Billing/billingAccounts/{billing_account_id}/providers/Microsoft.CostManagement/generateCostDetailsReport?api-version=2022-05-01\"\n","headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n","body = {\"metric\": \"ActualCost\", \"billingPeriod\": period_name}\n","\n","#ID unico de job\n","job_id = str(uuid.uuid4())\n","\n","#Fecha y hora de inicio\n","start_date_time = datetime.datetime.now()\n","\n","\n","#Crea solicitud de exportacion de datos a la API\n","response = requests.post(uri, headers=headers, data=json.dumps(body))\n","if response.status_code != 202:\n","    raise Exception(f\"Error al generar reporte: {response.text}\")\n","\n","export_url = response.headers.get(\"Location\")\n","print(f\"Export iniciado: {export_url}\")\n","\n","# Registrar log inicial en tabla delta del lakehouse\n","log_df = spark.createDataFrame([(job_id,period_name, customer_name, export_url, \"Starting\",start_date_time,start_date_time)],\n","                               [\"JobID\",\"Period\", \"Customer\", \"ExportURL\", \"Status\",\"Start\",\"End\"])\n","                            \n","log_df.write.mode(\"append\").saveAsTable(lakehouse_log_table)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5f329b04-d9f2-4e05-b5c9-97d2cd78439c"},{"cell_type":"markdown","source":["**Check the status of the export and log the result of the status**"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"8f4aee41-af20-4e21-86a6-de706b2d68a9"},{"cell_type":"code","source":["# ==========================\n","# 3. Polling para revisar estado de la exportación\n","# ==========================\n","status_export = \"Running\"\n","max_retries = 15\n","retry_count = 0\n","\n","\n","# Leemos tabla de log\n","delta_log_table = DeltaTable.forName(spark, lakehouse_log_table)\n","\n","while status_export == \"Running\" and retry_count < max_retries:\n","    try:\n","        status_response = requests.get(export_url, headers={\"Authorization\": f\"Bearer {token}\"})\n","        status_data = status_response.json()\n","        print(status_data)\n","        status_export = status_data.get(\"status\")\n","\n","        print(f\"Intento {retry_count + 1}: Estado = {status}\")\n","\n","        retry_count += 1\n","        #Espera 30 segundos entre cada intento\n","        time.sleep(30)\n","    except:\n","        print(\"The file is not ready, trying again\")\n","        time.sleep(30)\n","\n","if status_export == \"Completed\":\n","    #La exportacion concluyó\n","    status = \"File_exported\"\n","\n","    # Crear DataFrame con el nuevo estado\n","    update_df = spark.createDataFrame([(job_id,period_name, customer_name, export_url, status,start_date_time,start_date_time)],\n","                                        [\"JobID\",\"Period\", \"Customer\", \"ExportURL\", \"Status\",\"Start\",\"End\"])\n","\n","    # MERGE\n","    delta_log_table.alias(\"target\").merge(\n","        update_df.alias(\"source\"),\n","        \"target.JobID = source.JobID\"\n","    ).whenMatchedUpdate(set={\n","        \"Status\": \"source.Status\",\n","        \"ExportURL\": \"source.ExportURL\"\n","    }).execute()    \n","\n","else:\n","    # Crear DataFrame para status error\n","    status = \"Error_getting_file\"\n","    update_df = spark.createDataFrame([(job_id,period_name, customer_name, export_url, status,start_date_time,start_date_time)],\n","                                        [\"JobID\",\"Period\", \"Customer\", \"ExportURL\", \"Status\",\"Start\",\"End\"])\n","\n","    # MERGE\n","    delta_log_table.alias(\"target\").merge(\n","        update_df.alias(\"source\"),\n","        \"target.JobID = source.JobID\"\n","    ).whenMatchedUpdate(set={\n","        \"Status\": \"source.Status\",\n","        \"ExportURL\": \"source.ExportURL\"\n","    }).execute()   \n","\n","    #Lanzar exception para finalizar proceso\n","    raise Exception(f\"Export falló o no completó. Estado final: {status}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fa2c50cf-7ea9-4a49-be5e-ab1d571fed98"},{"cell_type":"markdown","source":["**Schema of the file exported by the API**"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"e42514f3-2f96-4c8d-891d-4fda712ebabf"},{"cell_type":"code","source":["#Esquema del archivo de consumo\n","\n","schema = StructType([\n","    StructField(\"InvoiceSectionName\", StringType(), True),\n","    StructField(\"AccountName\", StringType(), True),\n","    StructField(\"AccountOwnerId\", StringType(), True),\n","    StructField(\"SubscriptionId\", StringType(), True),\n","    StructField(\"SubscriptionName\", StringType(), True),\n","    StructField(\"ResourceGroup\", StringType(), True),\n","    StructField(\"ResourceLocation\", StringType(), True),\n","    StructField(\"Date\", StringType(), True),\n","    StructField(\"ProductName\", StringType(), True),\n","    StructField(\"MeterCategory\", StringType(), True),\n","    StructField(\"MeterSubCategory\", StringType(), True),\n","    StructField(\"MeterId\", StringType(), True),\n","    StructField(\"MeterName\", StringType(), True),\n","    StructField(\"MeterRegion\", StringType(), True),\n","    StructField(\"UnitOfMeasure\", StringType(), True),\n","    StructField(\"Quantity\", FloatType(), True),\n","    StructField(\"EffectivePrice\", FloatType(), True),\n","    StructField(\"CostInBillingCurrency\", FloatType(), True),\n","    StructField(\"CostCenter\", StringType(), True),\n","    StructField(\"ConsumedService\", StringType(), True),\n","    StructField(\"ResourceId\", StringType(), True),\n","    StructField(\"Tags\", StringType(), True),\n","    StructField(\"OfferId\", StringType(), True),\n","    StructField(\"AdditionalInfo\", StringType(), True),\n","    StructField(\"ServiceInfo1\", StringType(), True),\n","    StructField(\"ServiceInfo2\", StringType(), True),\n","    StructField(\"ResourceName\", StringType(), True),\n","    StructField(\"ReservationId\", StringType(), True),\n","    StructField(\"ReservationName\", StringType(), True),\n","    StructField(\"UnitPrice\", FloatType(), True),\n","    StructField(\"ProductOrderId\", StringType(), True),\n","    StructField(\"ProductOrderName\", StringType(), True),\n","    StructField(\"Term\", StringType(), True),\n","    StructField(\"PublisherType\", StringType(), True),\n","    StructField(\"PublisherName\", StringType(), True),\n","    StructField(\"ChargeType\", StringType(), True),\n","    StructField(\"Frequency\", StringType(), True),\n","    StructField(\"PricingModel\", StringType(), True),\n","    StructField(\"AvailabilityZone\", StringType(), True),\n","    StructField(\"BillingAccountId\", StringType(), True),\n","    StructField(\"BillingAccountName\", StringType(), True),\n","    StructField(\"BillingCurrencyCode\", StringType(), True),\n","    StructField(\"BillingPeriodStartDate\", StringType(), True),\n","    StructField(\"BillingPeriodEndDate\", StringType(), True),\n","    StructField(\"BillingProfileId\", StringType(), True),\n","    StructField(\"BillingProfileName\", StringType(), True),\n","    StructField(\"InvoiceSectionId\", StringType(), True),\n","    StructField(\"IsAzureCreditEligible\", StringType(), True),\n","    StructField(\"PartNumber\", StringType(), True),\n","    StructField(\"PayGPrice\", FloatType(), True),\n","    StructField(\"PlanName\", StringType(), True),\n","    StructField(\"ServiceFamily\", StringType(), True),\n","    StructField(\"CostAllocationRuleName\", StringType(), True),\n","    StructField(\"benefitId\", StringType(), True),\n","    StructField(\"benefitName\", StringType(), True)\n","])\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"34e68691-7eba-49cb-bd5c-8d9e1c30cabc"},{"cell_type":"markdown","source":["**Download generated CSV file by the API and then load it into a table of the lakehouse**"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"c5a13f6c-cde2-47a6-82d4-eb6c6130d88e"},{"cell_type":"code","source":["# ==========================\n","# 4. Descargar archivo CSV desde blobLink y cargar en Delta\n","# ==========================\n","\n","\n","# Extraer blobLink del JSON\n","blob_link = status_data.get(\"manifest\", {}).get(\"blobs\", [])[0].get(\"blobLink\")\n","if not blob_link:\n","    raise Exception(\"No se encontró blobLink en la respuesta del API.\")\n","\n","print(f\"Descargando archivo desde: {blob_link}\")\n","\n","# Descargar el archivo CSV\n","file_response = requests.get(blob_link)\n","if file_response.status_code != 200:\n","    raise Exception(f\"Error al descargar archivo: {file_response.text}\")\n","\n","# Guardar en Lakehouse local path\n","local_file = f\"/lakehouse/default/Files/{period_name}_costs.csv\"\n","with open(local_file, \"wb\") as f:\n","    f.write(file_response.content)\n","\n","print(f\"Archivo CSV descargado en: {local_file}\")\n","\n","# Leer CSV con Spark\n","cost_df = spark.read.format(\"csv\") \\\n","    .option(\"header\", \"true\") \\\n","    .option(\"quote\", \"\\\"\") \\\n","    .option(\"escape\", \"\\\"\") \\\n","    .schema(schema) \\\n","    .load(f\"Files/{period_name}_costs.csv\")\n","\n","# Escribir en Delta Table usada como staging en lakehouse\n","cost_df.write.mode(\"overwrite\").saveAsTable(lakehouse_cost_table)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e1b26ef7-3ea6-4b81-bde3-bf012ec9f6f7"},{"cell_type":"markdown","source":["**Update the log record to completed**"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"00567b19-adde-4586-b1a7-34063deaddca"},{"cell_type":"code","source":["# ==========================\n","# 5. Actualizar log como completado\n","# ==========================\n","\n","#fecha y hora de finalizacion\n","end_date_time = datetime.datetime.now()\n","\n","# Leemos tabla de log\n","delta_log_table = DeltaTable.forName(spark, lakehouse_log_table)\n","\n","# Crear DataFrame con el estado final\n","final_df = spark.createDataFrame([(job_id,period_name, customer_name, export_url, \"Completed\",start_date_time,end_date_time)],\n","                                 [\"JobID\",\"Period\", \"Customer\", \"ExportURL\", \"Status\",\"Start\",\"End\"])\n","\n","# MERGE para actualizar o insertar\n","delta_log_table.alias(\"target\").merge(\n","    final_df.alias(\"source\"),\n","    \"target.JobID = source.JobID\"\n",").whenMatchedUpdate(set={\n","    \"Status\": \"source.Status\",\n","    \"ExportURL\": \"source.ExportURL\",\n","    \"End\": \"source.End\",\n","}).whenNotMatchedInsert(values={\n","    \"JobID\": \"source.JobID\",\n","    \"Period\": \"source.Period\",\n","    \"Customer\": \"source.Customer\",\n","    \"ExportURL\": \"source.ExportURL\",\n","    \"Status\": \"source.Status\",\n","    \"Start\": \"source.Start\",\n","    \"End\": \"source.End\"\n","}).execute()\n","\n","print(\"Proceso completado y log actualizado con MERGE.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"778403ca-a439-4035-a636-aaa565e34a5b"},{"cell_type":"markdown","source":["**Pre processing of fields**"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"f4bf4bae-e4a8-4ea2-abb4-5f105f49f73c"},{"cell_type":"code","source":["# ==========================\n","# 6. Procesamiento de campos\n","# ==========================\n","from pyspark.sql.functions import col, current_date, year, month, dayofmonth\n","\n","# Función para extraer números de una columna\n","def extract_numbers(df, source_col, target_col=\"numeric_value\"):\n","    \"\"\"\n","    Extrae el primer número encontrado en la columna source_col y lo guarda en target_col.\n","    \"\"\"\n","    return df.withColumn(target_col, regexp_extract(col(source_col), r'(\\d+)', 1).cast(\"int\"))\n","\n","\n","# Convertir la columna \"Date\" a tipo date con formato explícito\n","cost_df = cost_df.withColumn(\"Date\", to_date(col(\"Date\"), \"MM/dd/yyyy\"))\n","\n","# Agrega campo UnitOfMeasureNumeric que sera utilizado en stored procedure del warehouse\n","cost_df = extract_numbers(cost_df, \"UnitOfMeasure\", \"UnitOfMeasureNumeric\")\n","\n","# Cuando el periodo es 0 entonces borramos los datos del dia actual ya que estan incompletos\n","\n","if period == 0:\n","    # Filtrar registros que NO son del día actual (para conservarlos)\n","    cost_df = cost_df.filter(\n","        ~(\n","            (year(col(\"Date\")) == year(current_date())) &\n","            (month(col(\"Date\")) == month(current_date())) &\n","            (dayofmonth(col(\"Date\")) == dayofmonth(current_date()))\n","        )\n","    )\n","\n","display(cost_df)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"b0739730-3e8f-4445-a3b8-0351b498d56b"},{"cell_type":"markdown","source":["**Write the dataframe into a warehouse table**"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"ddeb02de-6806-4184-883f-e6bcd247628e"},{"cell_type":"code","source":["import com.microsoft.spark.fabric\n","\n","#Escribir datos en tabla de warehouse\n","\n","cost_df.write.mode(\"append\").synapsesql(f\"{warehouse_name}.{warehouse_schema}.{warehouse_cost_table}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"94f146aa-a359-4c0d-b46f-73b956d95021"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"synapse_pyspark","language":null,"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"7547014a-7edd-4beb-80e4-0bb17c92959c","default_lakehouse_name":"consumoLH","default_lakehouse_workspace_id":"af0222ca-75c2-4c44-9cde-068b3fce8729","known_lakehouses":[{"id":"7547014a-7edd-4beb-80e4-0bb17c92959c"}]},"environment":{"environmentId":"dd48fb1d-5ef9-4975-8459-586c69cd4ad6","workspaceId":"af0222ca-75c2-4c44-9cde-068b3fce8729"}}},"nbformat":4,"nbformat_minor":5}